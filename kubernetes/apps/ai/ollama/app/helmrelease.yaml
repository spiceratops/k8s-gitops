---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ollama
  namespace: ai
spec:
  chartRef:
    kind: OCIRepository
    name: app-template
  dependsOn: []
  interval: 15m

  values:
    controllers:
      ollama:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: docker.io/ollama/ollama
              tag: 0.12.6
            env:
              TZ: ${TIMEZONE}
              OLLAMA_HOST: 0.0.0.0
              OLLAMA_ORIGINS: "*"
              OLLAMA_MODELS: /models
            resources:
              requests:
                cpu: 200m
                memory: 1Gi
                nvidia.com/gpu: 1
              limits:
                memory: 12Gi
                nvidia.com/gpu: 1
        pod:
          nodeSelector:
            nvidia.feature.node.kubernetes.io/gpu: "true"
          runtimeClassName: nvidia
    service:
      app:
        controller: ollama
        type: LoadBalancer
        loadBalancerIP: ${OLLAMA_LB_IP}
        externalTrafficPolicy: Local
        annotations:
          external-dns.alpha.kubernetes.io/hostname: ollama.${SECRET_DOMAIN_INTERNAL}
        ports:
          http:
            port: 11434
    persistence:
      models:
        enabled: true
        existingClaim: nfs-ai-models
        advancedMounts:
          ollama:
            app:
              - path: /models
      config:
        enabled: true
        existingClaim: ollama
        globalMounts:
          - path: /root/.ollama
